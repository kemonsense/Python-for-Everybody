{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file name: \n",
      "445833.0\n"
     ]
    }
   ],
   "source": [
    "#Week 2\n",
    "#Extracting Data With Regular Expressions\n",
    "\n",
    "import re\n",
    "\n",
    "fname = input(\"Enter file name: \")\n",
    "if len(fname) < 1 : fname = \"rg42.txt\"\n",
    "\n",
    "fh = open(fname)\n",
    "\n",
    "numlist = []\n",
    "\n",
    "for line in fh:\n",
    "    line = line.rstrip()\n",
    "    extract = re.findall(\"([0-9]+)\", line)\n",
    "\n",
    "    if len(extract) < 1 : continue\n",
    "\n",
    "    for i in extract:\n",
    "        num = float(i)\n",
    "        numlist.append(num)\n",
    "\n",
    "print(sum(numlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week 3\n",
    "\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week 4.1\n",
    "\n",
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "numlist = []\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    num= int(tag.contents[0])\n",
    "    numlist.append(num)\n",
    "\n",
    "print(sum(numlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week 4.2\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# get user input for Url to search\n",
    "url = input('Enter Url- ')\n",
    "\n",
    "# get user input for how many times to search\n",
    "count = int(input('Enter count:'))\n",
    "\n",
    "# get user input for which url to click on\n",
    "position = int(input('Enter position:'))-1\n",
    "\n",
    "while count >= 0:\n",
    "    # re-reads the current url\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    # creates a new soup object\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # searches the page for all <a> tags\n",
    "    tags = soup('a')\n",
    "    print(\"Retrieving: \", url)\n",
    "    # upates the current url\n",
    "    url = tags[position].get(\"href\", None)\n",
    "    count = count - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week 5\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "link = input('Enter location: ')\n",
    "html = urllib.request.urlopen(link).read().decode()\n",
    "print('Retrieving', link)\n",
    "print('Retrieved', len(html), 'characters')\n",
    "\n",
    "\n",
    "#data calculation\n",
    "cn = 0\n",
    "sm = 0\n",
    "data = ET.fromstring(html)\n",
    "tags = data.findall('comments/comment')\n",
    "\n",
    "for tag in tags:\n",
    "    cn += 1\n",
    "    sm += int(tag.find('count').text)\n",
    "\n",
    "print('Count:', cn)\n",
    "print('Sum:', sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_1023874.json\n",
      "Retrieving http://py4e-data.dr-chuck.net/comments_1023874.json\n",
      "Retrieved 2718 characters\n",
      "Count: 50\n",
      "Sum: 2574\n"
     ]
    }
   ],
   "source": [
    "#Week 6.1\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "#Data collection\n",
    "link = input('Enter location: ')\n",
    "print('Retrieving', link)\n",
    "\n",
    "html = urllib.request.urlopen(link).read().decode()\n",
    "print('Retrieved', len(html), 'characters')\n",
    "\n",
    "try:\n",
    "    js = json.loads(html)\n",
    "except:\n",
    "    js = None\n",
    "\n",
    "cn = 0\n",
    "sm = 0\n",
    "for item in js['comments']:\n",
    "    cn += 1\n",
    "    sm += int(item['count'])\n",
    "\n",
    "print('Count:', cn)\n",
    "print('Sum:', sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: South Federal University\n",
      "Retrieving http://py4e-data.dr-chuck.net/json?address=South+Federal+University\n",
      "Retrieved 67 characters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c6c6a269bd6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mplaceId\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'place_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Place id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaceId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Week 6.2\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)\n",
    "    placeId = js['results'][0]['place_id']\n",
    "    print('Place id', placeId)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
